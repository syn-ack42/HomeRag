from fastapi import FastAPI, UploadFile, Request
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles

from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.llms import Ollama
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from pathlib import Path
import shutil, os
from pypdf import PdfReader

DATA_PATH = Path(os.environ.get("DATA_PATH", "/data"))
DB_PATH = Path(os.environ.get("DB_PATH", "/chroma"))
OLLAMA_URL = os.environ.get("OLLAMA_URL", "http://localhost:11434")
MODEL = os.environ.get("MODEL", "mistral")

app = FastAPI(title="Local RAG Chat")

app.mount("/static", StaticFiles(directory="static"), name="static")

def extract_text_from_pdf(pdf_path: Path) -> str:
    text = ""
    reader = PdfReader(str(pdf_path))
    for page in reader.pages:
        text += page.extract_text() or ""
    return text

def load_documents():
    docs = []
    for file in DATA_PATH.glob("*"):
        if file.suffix.lower() == ".pdf":
            docs.append({"page_content": extract_text_from_pdf(file), "metadata": {"source": file.name}})
        elif file.suffix.lower() in [".txt", ".md"]:
            docs.append({"page_content": file.read_text(errors="ignore"), "metadata": {"source": file.name}})
    return docs

def build_db():
    docs = load_documents()
    if not docs:
        return
    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    chunks = splitter.create_documents([d["page_content"] for d in docs])
    embeddings = OllamaEmbeddings(base_url=OLLAMA_URL, model="nomic-embed-text")
    vectordb = Chroma.from_documents(chunks, embeddings, persist_directory=str(DB_PATH))
    vectordb.persist()

@app.on_event("startup")
def startup_event():
    if not (DB_PATH / "chroma.sqlite3").exists():
        build_db()

@app.get("/", response_class=HTMLResponse)
def index():
    with open("static/index.html") as f:
        return HTMLResponse(f.read())

@app.post("/upload")
async def upload(file: UploadFile):
    target = DATA_PATH / file.filename
    with open(target, "wb") as f:
        shutil.copyfileobj(file.file, f)
    return {"status": "uploaded", "file": file.filename}

@app.post("/rebuild")
def rebuild():
    build_db()
    return {"status": "reindexed"}

@app.post("/ask")
async def ask(request: Request):
    data = await request.json()
    question = data.get("q")
    embeddings = OllamaEmbeddings(base_url=OLLAMA_URL, model="nomic-embed-text")
    vectordb = Chroma(persist_directory=str(DB_PATH), embedding_function=embeddings)

#    retriever = vectordb.as_retriever(search_kwargs={"k": 4})
#    llm = Ollama(base_url=OLLAMA_URL, model=MODEL)
#    qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
#    result = qa.invoke(question)

    retriever = vectordb.as_retriever(search_kwargs={"k": 4})
    llm = Ollama(base_url=OLLAMA_URL, model=MODEL)

    prompt = ChatPromptTemplate.from_template(
        "Use only the following context to answer the question.\n\n"
        "Context:\n{context}\n\nQuestion: {question}"
    )

    # Assemble the pipeline
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    result = rag_chain.invoke(question)
    return {"answer": result}
