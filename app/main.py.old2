from fastapi import FastAPI, UploadFile, Request
from fastapi.responses import HTMLResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.llms import Ollama

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from pathlib import Path
from pypdf import PdfReader
import shutil
import os
import json

DATA_PATH = Path(os.environ.get("DATA_PATH", "/data"))
DB_PATH = Path(os.environ.get("DB_PATH", "/chroma"))
OLLAMA_URL = os.environ.get("OLLAMA_URL", "http://ollama:11434")
MODEL = os.environ.get("MODEL", "mistral")
CONFIG_PATH = Path(os.environ.get("CONFIG_PATH", "/config"))
PROMPT_FILE = CONFIG_PATH / "prompt.txt"

app = FastAPI(title="cadis KB Chat")
app.mount("/static", StaticFiles(directory="static"), name="static")


DEFAULT_PROMPT = """
You are a helpful assistant.
Use ONLY the provided context to answer.
If something is not in the context, say you don't know.
Never hallucinate.
"""

def load_system_prompt():
    CONFIG_PATH.mkdir(parents=True, exist_ok=True)
    if PROMPT_FILE.exists():
        return PROMPT_FILE.read_text()
    return DEFAULT_PROMPT

def save_system_prompt(text: str):
    CONFIG_PATH.mkdir(parents=True, exist_ok=True)
    PROMPT_FILE.write_text(text)


SYSTEM_INSTRUCTIONS = """
You are a helpful and concise assistant.
Always answer based on the provided context.
If the context does not contain the info, say you don't know.
Never hallucinate.
"""




def extract_text_from_pdf(pdf_path: Path) -> str:
    text = ""
    reader = PdfReader(str(pdf_path))
    for page in reader.pages:
        txt = page.extract_text() or ""
        text += txt + "\n"
    return text


def load_documents():
    docs = []
    for file in DATA_PATH.glob("*"):
        if file.suffix.lower() == ".pdf":
            docs.append({
                "page_content": extract_text_from_pdf(file),
                "metadata": {"source": file.name}
            })
        elif file.suffix.lower() in (".txt", ".md"):
            docs.append({
                "page_content": file.read_text(errors="ignore"),
                "metadata": {"source": file.name}
            })
    return docs


def build_db():
    docs = load_documents()
    if not docs:
        return

    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    chunks = splitter.create_documents([d["page_content"] for d in docs],
                                       metadatas=[d["metadata"] for d in docs])

    embeddings = OllamaEmbeddings(base_url=OLLAMA_URL, model="nomic-embed-text")

    vectordb = Chroma.from_documents(
        chunks,
        embeddings,
        persist_directory=str(DB_PATH)
    )
    vectordb.persist()


@app.on_event("startup")
def startup_event():
    if not (DB_PATH / "chroma.sqlite3").exists():
        build_db()


@app.get("/", response_class=HTMLResponse)
def index():
    with open("static/index.html") as f:
        return HTMLResponse(f.read())


@app.post("/upload")
async def upload(file: UploadFile):
    target = DATA_PATH / file.filename
    with open(target, "wb") as f:
        shutil.copyfileobj(file.file, f)
    return {"status": "uploaded", "file": file.filename}


@app.post("/rebuild")
def rebuild():
    build_db()
    return {"status": "reindexed"}

@app.get("/prompt")
def get_prompt():
    return {"prompt": load_system_prompt()}

@app.post("/prompt")
async def post_prompt(request: Request):
    data = await request.json()
    save_system_prompt(data.get("prompt", ""))
    return {"status": "saved"}


@app.post("/ask_stream")
async def ask_stream(request: Request):
    data = await request.json()
    question = data.get("q")

    embeddings = OllamaEmbeddings(base_url=OLLAMA_URL, model="nomic-embed-text")
    vectordb = Chroma(
        persist_directory=str(DB_PATH),
        embedding_function=embeddings
    )

    retriever = vectordb.as_retriever(search_kwargs={"k": 4})
    llm = Ollama(base_url=OLLAMA_URL, model=MODEL)


    # Prompt
    def build_prompt():
        system = load_system_prompt()
        return ChatPromptTemplate.from_template(
            system + "\n\nContext:\n{context}\n\nQuestion:\n{question}"
        )

    def join_docs(docs):
        return "\n\n---\n\n".join(doc.page_content for doc in docs)

    prompt = build_prompt()
    rag_chain = (
        {
            "context": retriever | (lambda docs: join_docs(docs)),
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    async def stream():

        # Then tokens
        async for token in rag_chain.astream(question):
            yield json.dumps({"type": "token", "token": token}) + "\n"

    return StreamingResponse(stream(), media_type="text/plain")

